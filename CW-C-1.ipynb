{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f25c1a3-f7b1-4c69-9b5f-185ff9f30543",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#create catelog, schema & volume by UI\n",
    "#download txt file under the volume\n",
    "import requests\n",
    "\n",
    "url = \"https://www.gutenberg.org/cache/epub/51355/pg51355.txt\"\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "\n",
    "with open(\n",
    "    \"/Volumes/textcontent/textschema/textvolume/iliad.txt\",\n",
    "    \"wb\"\n",
    ") as f:\n",
    "    f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bba8df6-a6df-44df-ab4f-2e7fd4caf06f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.text(\n",
    "    \"/Volumes/textcontent/textschema/textvolume/iliad.txt\"\n",
    ")\n",
    "display(\n",
    "    df.limit(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30535244-fb03-4291-8cda-92bad5d82ca3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode\n",
    "\n",
    "words_df = df.select(\n",
    "    explode(\n",
    "        split(df[\"value\"], \" \")\n",
    "    ).alias(\"word\")\n",
    ")\n",
    "display(\n",
    "    words_df.limit(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab5852ee-e6ab-4ef6-a41c-0d6986efa891",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60f3cc1c-10dc-4278-91bf-55556be46a38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#change the stopword from doc\n",
    "\n",
    "stopwords = ['call', 'upon', 'still', 'nevertheless', 'down', 'every', 'forty', \n",
    "'‘re', 'always', 'whole', 'side', \"n't\", 'now', 'however', 'an', 'show', 'least', \n",
    "'give', 'below', 'did', 'sometimes', 'which', \"'s\", 'nowhere', 'per', 'hereupon', \n",
    "'yours', 'she', 'moreover', 'eight', 'somewhere', 'within', 'whereby', 'few', \n",
    "'has', 'so', 'have', 'for', 'noone', 'top', 'were', 'those', 'thence', 'eleven', \n",
    "'after', 'no', '’ll', 'others', 'ourselves', 'themselves', 'though', 'that', \n",
    "'nor', 'just', '’s', 'before', 'had', 'toward', 'another', 'should', 'herself', \n",
    "'and', 'these', 'such', 'elsewhere', 'further', 'next', 'indeed', 'bottom', \n",
    "'anyone', 'his', 'each', 'then', 'both', 'became', 'third', 'whom', '‘ve', 'mine', \n",
    "'take', 'many', 'anywhere', 'to', 'well', 'thereafter', 'besides', 'almost', \n",
    "'front', 'fifteen', 'towards', 'none', 'be', 'herein', 'two', 'using', 'whatever', \n",
    "'please', 'perhaps', 'full', 'ca', 'we', 'latterly', 'here', 'therefore', 'us', \n",
    "'how', 'was', 'made', 'the', 'or', 'may', '’re', 'namely', \"'ve\", 'anyway', \n",
    "'amongst', 'used', 'ever', 'of', 'there', 'than', 'why', 'really', 'whither', \n",
    "'in', 'only', 'wherein', 'last', 'under', 'own', 'therein', 'go', 'seems', '‘m', \n",
    "'wherever', 'either', 'someone', 'up', 'doing', 'on', 'rather', 'ours', 'again', \n",
    "'same', 'over', '‘s', 'latter', 'during', 'done', \"'re\", 'put', \"'m\", 'much', \n",
    "'neither', 'among', 'seemed', 'into', 'once', 'my', 'otherwise', 'part', \n",
    "'everywhere', 'never', 'myself', 'must', 'will', 'am', 'can', 'else', 'although', \n",
    "'as', 'beyond', 'are', 'too', 'becomes', 'does', 'a', 'everyone', 'but', 'some', \n",
    "'regarding', '‘ll', 'against', 'throughout', 'yourselves', 'him', \"'d\", 'it', \n",
    "'himself', 'whether', 'move', '’m', 'hereafter', 're', 'while', 'whoever', 'your', \n",
    "'first', 'amount', 'twelve', 'serious', 'other', 'any', 'off', 'seeming', 'four', \n",
    "'itself', 'nothing', 'beforehand', 'make', 'out', 'very', 'already', 'various', \n",
    "'until', 'hers', 'they', 'not', 'them', 'where', 'would', 'since', 'everything', \n",
    "'at', 'together', 'yet', 'more', 'six', 'back', 'with', 'thereupon', 'becoming', \n",
    "'around', 'due', 'keep', 'somehow', 'n‘t', 'across', 'all', 'when', 'i', 'empty', \n",
    "'nine', 'five', 'get', 'see', 'been', 'name', 'between', 'hence', 'ten', \n",
    "'several', 'from', 'whereupon', 'through', 'hereby', \"'ll\", 'alone', 'something', \n",
    "'formerly', 'without', 'above', 'onto', 'except', 'enough', 'become', 'behind', \n",
    "'’d', 'its', 'most', 'n’t', 'might', 'whereas', 'anything', 'if', 'her', 'via', \n",
    "'fifty', 'is', 'thereby', 'twenty', 'often', 'whereafter', 'their', 'also', \n",
    "'anyhow', 'cannot', 'our', 'could', 'because', 'who', 'beside', 'by', 'whence', \n",
    "'being', 'meanwhile', 'this', 'afterwards', 'whenever', 'mostly', 'what', 'one', \n",
    "'nobody', 'seem', 'less', 'do', '‘d', 'say', 'thus', 'unless', 'along', \n",
    "'yourself', 'former', 'thru', 'he', 'hundred', 'three', 'sixty', 'me', 'sometime', \n",
    "'whose', 'you', 'quite', '’ve', 'about', 'even', 'thou', 'thy']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1738f32b-a149-47d2-b9f5-6bb15eea2c2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    spark.createDataFrame(\n",
    "        [(word,) for word in stopwords[:10]],\n",
    "        [\"stopword\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8aadcef6-a468-4162-b816-194748f39943",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "\n",
    "# Tokenize the string column 'word' into an array of words\n",
    "tokenizer = Tokenizer(\n",
    "    inputCol=\"word\",\n",
    "    outputCol=\"words\"\n",
    ")\n",
    "tokenized_df = tokenizer.transform(words_df)\n",
    "\n",
    "# Remove stopwords from the array column\n",
    "stopwordremover = StopWordsRemover(\n",
    "    inputCol=\"words\",\n",
    "    outputCol=\"filtered_words\",\n",
    "    stopWords=stopwords,\n",
    "    caseSensitive=False\n",
    ")\n",
    "filtered_df = stopwordremover.transform(tokenized_df)\n",
    "\n",
    "#display(filtered_df.limit(20))\n",
    "\n",
    "#filtered_df.select(\"words\", \"filtered_words\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87c2a8dc-3670-4057-a3c7-8c56cc953ea0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array_join\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "final_df = filtered_df.withColumn(\"cleaned_text\", array_join(col(\"filtered_words\"), \" \"))\n",
    "display(final_df.limit(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23102b7a-f057-4d46-a9ba-c6aa3028d52c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69dca8e9-89d4-4a9a-8bee-068900a46581",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764276879702}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "repaced by next step"
    }
   },
   "outputs": [],
   "source": [
    "#this can be replaced by map/reduce step\n",
    "word_counts = final_df.groupBy(\"cleaned_text\").count()\n",
    "display(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dbcfd9d-7739-4ecc-ad10-cbd8c955ea4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Map: assign count 1 to each cleaned_text\n",
    "#mapped_df = final_df.withColumn(\"count\", col(\"cleaned_text\").isNotNull().cast(\"int\"))\n",
    "\n",
    "#add count only to those that are not null or empty\n",
    "mapped_df = final_df.filter(\n",
    "    (col(\"cleaned_text\").isNotNull()) &\n",
    "    (col(\"cleaned_text\") != \"\")\n",
    ").withColumn(\n",
    "    \"count\",\n",
    "    col(\"cleaned_text\").isNotNull().cast(\"int\")\n",
    ")\n",
    "display(mapped_df.limit(10))\n",
    "\n",
    "# Reduce: group by cleaned_text and sum the counts\n",
    "reduced_df = mapped_df.groupBy(\"cleaned_text\").sum(\"count\").withColumnRenamed(\"sum(count)\", \"count\")\n",
    "\n",
    "display(reduced_df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94065252-f3dd-4dbc-ab59-8bc31e61396f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sorted_df = reduced_df.orderBy(\n",
    "    col(\"count\").desc()\n",
    ")\n",
    "display(sorted_df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f87f469b-8bfc-41e4-a583-e87ea6cc16c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#convert the df into a table\n",
    "sorted_df.select(\n",
    "    col(\"cleaned_text\").alias(\"term\"),\n",
    "    col(\"count\").alias(\"frequency\")\n",
    ").write.mode(\"overwrite\").saveAsTable(\"wordcount_table\")\n",
    "\n",
    "table_df = spark.read.table(\"wordcount_table\")\n",
    "display(table_df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e08752f0-3bd2-425a-916b-d15ee0819f27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZGYgPSBzcGFyay5yZWFkLnRhYmxlKCJ3b3JkY291bnRfdGFibGUiKQpkaXNwbGF5KGRmLm9yZGVyQnkoCiAgICBjb2woImZyZXF1ZW5jeSIpLmRlc2MoKQopLmxpbWl0KDI1KSk=\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView63d4c6c\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView63d4c6c\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView63d4c6c\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView63d4c6c) SELECT `term`,SUM(`frequency`) `column_9f69c819179`,`term` FROM q GROUP BY `term`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView63d4c6c\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "series": {
             "column": "term",
             "id": "column_9f69c819182"
            },
            "x": {
             "column": "term",
             "id": "column_9f69c819178"
            },
            "y": [
             {
              "column": "frequency",
              "id": "column_9f69c819179",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "hideXAxis": false,
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "reverseX": false,
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_9f69c819179": {
             "name": "frequency",
             "type": "column",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": false,
           "sortY": true,
           "swappedAxes": true,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "title": {
             "text": "Words"
            },
            "type": "-"
           },
           "yAxis": [
            {
             "title": {
              "text": "Frequency"
             },
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "01d2d262-017e-44e0-9f68-0434903dc76f",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 17.5,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "term",
           "type": "column"
          },
          {
           "column": "term",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "term",
           "type": "column"
          },
          {
           "alias": "column_9f69c819179",
           "args": [
            {
             "column": "frequency",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          },
          {
           "column": "term",
           "type": "column"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.read.table(\"wordcount_table\")\n",
    "display(df.orderBy(\n",
    "    col(\"frequency\").desc()\n",
    ").limit(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cbec13c-91f8-4ebc-9dd8-1e1fbff37909",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "empty_count = final_df.filter(\n",
    "    col(\"cleaned_text\") == \"\"\n",
    ").count()\n",
    "print(empty_count)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "CW-C-1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
